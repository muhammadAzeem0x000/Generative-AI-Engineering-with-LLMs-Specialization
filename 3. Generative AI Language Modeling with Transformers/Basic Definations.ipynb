{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and norm:\n",
    "A process that enhances the depth of the model while mitigating potential issues with gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argmax process:\n",
    "A process that helps obtain the translated word's token index and further replace the self-attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mechanism:\n",
    "A neural network component that weighs input elements during processing and focuses on relevant parts of output generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoregressive model:\n",
    "A model that facilitates sequence generation by anticipating each new token based on the sequence's preceding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT:\n",
    "An open-source, deeply bidirectional, unsupervised language representation pretrained using a plain text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual embeddings:\n",
    "A type of embedding that aptly describes how the transformer processes the input word embeddings by accounting for the context in which each word occurs within the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader:\n",
    "A utility in a machine learning framework that collects operational data from data sources at regular intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder models:\n",
    "A type of network architecture commonly used in sequence-to-sequence tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning:\n",
    "A supervised process that optimizes the initially trained GPT model for specific tasks, like QA classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative pre-training (GPT):\n",
    "A self-supervised model that involves training a decoder to predict the subsequent token or word in a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models:\n",
    "A model that predicts words by analyzing the previous text, where context length acts as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked language modeling:\n",
    "A model that learns tasks by reconstructing sentences with words that have been obscured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking:\n",
    "A function used to perform masking operations on tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention:\n",
    "An attention that executes several scaled dot-product attention processes in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Sentence Prediction:\n",
    "A training that enables the model to understand how sentences relate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding approach:\n",
    "An encoding approach that maps categorical features to binary representations, which are used to map the feature in a matrix or vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonality:\n",
    "An object-relational database in which the various parts work naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding:\n",
    "A technique in natural language processing and deep learning used to embed sequence positions into data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch:\n",
    "A software-based open-source deep learning framework used to build neural networks, combining Torch's machine learning library with a Python-based high-level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python dictionary:\n",
    "A built-in data structure that stores key-value pairs and facilitates efficient lookup and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning from Human Feedback (RLHF):\n",
    "A model that represents a fine-tuning approach and enhances model performance on specific tasks, proving particularly effective in chatbot development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled dot-product attention:\n",
    "A mechanism within the transformer model fundamentally involves a series of matrix multiplications incorporating queries, keys, and a scaling factor to prevent the dot product from becoming too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention mechanism:\n",
    "A mechanism that calculates weights for every word in a sentence, enabling the model to predict words that are likely to be used in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic:\n",
    "A term used in natural language processing referring to language's meaning and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple language modeling:\n",
    "A neural network architecture crucial for natural language understanding, predicting subsequent words in sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function:\n",
    "A mathematical function that converts raw scores into probabilities in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "The process of converting the words in the prompt into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer model:\n",
    "A model that can translate text and speech in near real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector:\n",
    "A mathematical object represented by a group of numbers commonly used in machine learning algorithms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

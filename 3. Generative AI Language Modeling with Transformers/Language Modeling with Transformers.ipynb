{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset():\n",
    "This code loads the IMDB data set and initializes iterators for both the training and validation sets. It then creates an iterator data_itr from the training iterator train_iter and retrieves the next data sample using the next() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "train_iter, val_iter= IMDB()\n",
    "data_itr=iter(train_iter)\n",
    "next(data_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Pipeline ():\n",
    "You can utilize PyTorch's torchtext library to streamline the text processing pipeline for NLP tasks. Specifically, get_tokenizer(\"basic_english\") from torchtext.data.utils is used for tokenizing the text data into a list of tokens. This tokenization process is essential for converting raw text into a format that your model can interpret. Furthermore, build_vocab_from_iterator, another utility from torchtext.vocab, is leveraged to construct the vocabulary from the tokenized text. This function iterates through the tokenized data, capturing the unique tokens and associating them with indices, including special symbols like <unk> for unknown tokens, <pad> for padding, and <eos> for end-of-sentence markers. By specifying specials and special_first=True, you ensure these special tokens are prioritized and properly indexed in your vocabulary, setting the groundwork for effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, EOS_IDX = 0, 1, 2\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<eos>']\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "def yield_tokens(data_iter):\n",
    "    for _, data_sample in data_iter:\n",
    "        yield tokenizer(data_sample) + ['<eos>']\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "specials=special_symbols, special_first=True)\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "text_to_index = lambda text: [vocab(token) for token in\n",
    "tokenizer(text)] + [EOS_IDX]\n",
    "index_to_text = lambda seq_en: \" \".join([vocab.get_itos()[index] for\n",
    "index in seq_en if index != EOS_IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating data for next token prediction():\n",
    "This code snippet demonstrates a critical step in preparing data for training a language model: generating input-target pairs, where each pair is used for the next token prediction. The function get_sample takes two parameters: block_size, which defines the maximum length of the text sample, and text, the input text from which the sample is generated. To create a diverse training set, torch.randint is used to select a random starting point within the text. This randomness ensures that the model encounters different segments of the text during training, which is vital for learning a robust representation of the language. The selected text segment (src_sequence) and its immediate next token (tgt_sequence) form a pair used to train the model to predict the next token given a sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_sample(block_size, text):\n",
    "# Determine the length of the input text\n",
    "sample_leg = len(text)\n",
    "# Calculate the stopping point for randomly selecting a sample\n",
    "# This ensures the selected sample doesn't exceed the text length\n",
    "random_sample_stop = sample_leg - block_size\n",
    "# Check if a random sample can be taken (if the text is longer than block_size)\n",
    "if random_sample_stop >= 1:\n",
    "# Randomly select a starting point for the sample\n",
    "random_start = torch.randint(low=0, high=random_sample_stop,\n",
    "size=(1,)).item()\n",
    "# Define the endpoint of the sample\n",
    "stop = random_start + block_size\n",
    "# Create the input and target sequences\n",
    "src_sequence = text[random_start:stop]\n",
    "tgt_sequence= text[random_start + 1:stop + 1]\n",
    "# Handle the case where the text length is exactly equal to or lesser than the block_size\n",
    "else:\n",
    "# Start from the beginning and use the entire text\n",
    "random_start = 0\n",
    "stop = sample_leg\n",
    "src_sequence= text[random_start:stop]\n",
    "tgt_sequence = text[random_start + 1:stop]\n",
    "# Append an empty string to maintain sequence alignment\n",
    "tgt_sequence.append( '<|endoftext|>')\n",
    "return src_sequence, tgt_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Src, Tgt) pairs ():\n",
    "This code snippet generates a sample pair for training a language model, where block_size determines the maximum length of the sample and text represents the input text. It then prints the source sequence (src_sequences) and the corresponding target sequence (tgt_sequence). The function get_sample randomly selects a segment of the text of length block_size, ensuring proper alignment between the source and target sequences. If the text is shorter than block_size, it uses the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "block_size=10\n",
    "src_sequences, tgt_sequence=get_sample( block_size, text)\n",
    "print(src_sequences)\n",
    "print(tgt_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collate function():\n",
    "This code defines a function collate_batch to prepare batches of input-source and target sequences for training a language model. It iterates over a batch of data, generates source and target sequences using the get_sample function with a specified block size (BLOCK_SIZE), tokenizes the text using a tokenizer, and converts tokens to indices using a vocabulary. The sequences are then converted to PyTorch tensors and appended to the respective source and target batches. Finally, the function pads sequences within the batch to ensure uniform length and returns the processed batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "BLOCK_SIZE=30\n",
    "def collate_batch(batch):\n",
    "src_batch, tgt_batch = [], []\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for _,_textt in batch:\n",
    "src_sequence,tgt_sequence=get_sample(BLOCK_SIZE,tokenizer(_textt))\n",
    "src_sequence=vocab(src_sequence)\n",
    "tgt_sequence=vocab(tgt_sequence)\n",
    "src_sequence= torch.tensor(src_sequence, dtype=torch.int64)\n",
    "tgt_sequence = torch.tensor(tgt_sequence, dtype=torch.int64)\n",
    "src_batch.append(src_sequence)\n",
    "tgt_batch.append(tgt_sequence)\n",
    "src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=False)\n",
    "return src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
    "BATCH_SIZE=1\n",
    "dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader= DataLoader(val_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking future tokens: Causal mask():\n",
    "These functions create masks used in transformer-based models for self-attention:\n",
    "\n",
    "generate_square_subsequent_mask(sz, device=DEVICE): Generates a mask to prevent attending to subsequent positions in a sequence during self-attention.\n",
    "\n",
    "create_mask(src, device=DEVICE): Creates a mask for the source sequence to ensure that each position can only attend to previous positions during self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "mask = (torch.triu(torch.ones((sz, sz), device=device)) ==1).transpose(0, 1)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "return mask\n",
    "def create_mask(src,device=DEVICE):\n",
    "src_seq_len = src.shape[0]\n",
    "src_mask=\n",
    "nn.Transformer.generate_square_subsequent_mask(None,src_seq_len).to(device)\n",
    "return src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding mask():\n",
    "The code generates a boolean mask, src_padding_mask, indicating the presence of padding tokens (True) in the source sequence src. Each True value corresponds to a padding token, while False represents non-padding tokens. The mask is structured to align with the sequence dimensions for proper masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "src.T:tensor([[ 1, 1, 1, 1, 6, 169, 438, 709..\n",
    "padding_mask:tensor([[ True, True, True, True, False, False, False, False, .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom GPT model architecture():\n",
    "This forward pass function applies positional embeddings to input embeddings, incorporates source masks if provided, and passes the input through a transformer encoder. Finally, it passes the output through a linear layer (lm_head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forward(self,x,src_mask=None,key_padding_mask=None):\n",
    "# Add positional embeddings to the input embeddings\n",
    "x = self.embed(x)* math.sqrt(self.embed_size)\n",
    "x = self.positional_encoding(x)\n",
    "if src_mask is None:\n",
    "src_mask, src_padding_mask = create_mask(x)\n",
    "output = self.transformer_encoder(x, src_mask, key_padding_mask)\n",
    "x = self.lm_head(x)\n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a small instance of the model:\n",
    "This code snippet initializes a custom GPT (Generative Pre-trained Transformer) model with specified parameters such as embedding size, number of transformer encoder layers, number of attention heads, vocabulary size, and dropout probability. The model is then moved to the specified device (DEVICE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab) # size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nlayers = 2 # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2 # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2 # dropout probability\n",
    "model = CustomGPTModel(embed_size=emsize, num_heads=nhead, num_layers=nlayers, vocab_size = ntokens,dropout=dropout).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the loss:\n",
    "During loss calculation, the encoder model generates a source and a target. During prediction, the decoder model generates logits Class 1 and Class 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "src= srctgt[0]\n",
    "tgt= srctgt[1]\n",
    "logits = model(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_fn:\n",
    "In preparation for loss calculation, you can see the reshaping of logits, where each row corresponds to the prediction for a token, spanning across both the sequence and the batch dimensions. You can reshape the target tensor so that its elements correspond correctly to the logits. This process ensures that every row from the logits aligns with the appropriate target outcomes for accurate loss estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "loss = loss_fn(logits_flat, tgt.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training ():\n",
    "The training process is similar to other models, such as convolutional neural networks or CNNs, recurrent neural networks or RNNs, transformers, and generative models. It uses the modified loss shape and other functions, such as validation and checkpoint saving, that help in the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lr = 5 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10000, gamma=0.9)\n",
    "def train(model: nn.Module,train_data) -> None:\n",
    "model.train() # turn on train mode\n",
    "total_loss = 0.\n",
    "log_interval = 10000\n",
    "start_time = time.time()\n",
    "num_batches = len(list(train_data)) // block_size\n",
    "for batch,srctgt in enumerate(train_data):\n",
    "src= srctgt[0]\n",
    "tgt= srctgt[1]\n",
    "logits = model(src,src_mask=None, key_padding_mask=None)\n",
    "logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "loss = loss_fn(logits_flat, tgt.reshape(-1))\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "optimizer.step()\n",
    "total_loss += loss.item()\n",
    "if (batch % log_interval == 0 and batch > 0) or batch==42060:\n",
    "lr = scheduler.get_last_lr()[0]\n",
    "ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "#cur_loss = total_loss / log_interval\n",
    "cur_loss = total_loss / batch\n",
    "ppl = math.exp(cur_loss)\n",
    "print(f'| epoch {epoch:3d} | {batch//block_size:5d}/{num_batches:5d}\n",
    "batches | '\n",
    "f'lr {lr:02.4f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "#total_loss = 0\n",
    "start_time = time.time()\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Validation function:\n",
    "The validation function is important to assess the model on a separate, invisible data set during training to gauge generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, validation_loader, loss_fn):\n",
    "   model.eval()\n",
    "   total_loss = 0\n",
    "   with torch.no_grad():\n",
    "     for src, tgt in validation_loader:\n",
    "       src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "       logits = model(src)\n",
    "       loss = loss_fn(logits.reshape(-1, logits.shape[-1]),\n",
    "tgt.reshape(-1))\n",
    "       total_loss += loss.item()\n",
    "   return total_loss / len(validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint saving function:\n",
    "The checkpoint saving function is useful for saving the model's state after certain intervals or under specific conditions, like improved validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer,\n",
    "filename=\"my_checkpoint.pth\"):\n",
    "   checkpoint = {\n",
    "     \"model_state_dict\": model.state_dict(),\n",
    "     \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "   }\n",
    "   torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: Evaluate function:\n",
    "The 'evaluate' function measures the performance of the model by computing its average loss in the validation data set. However, the trained model is useful to generate inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data) -> float:\n",
    "model.eval() # turn on evaluation mode\n",
    "total_loss = 0.\n",
    "with torch.no_grad():\n",
    "for src,tgt in eval_data:\n",
    "tgt = tgt.to(DEVICE)\n",
    "#seq_len = src.size(0)\n",
    "logits = model(src,src_mask=None, key_padding_mask=None)\n",
    "total_loss += loss_fn(logits.reshape(-1, logits.shape[-1]),\n",
    "tgt.reshape(-1)).item() return total_loss / (len(list(eval_data)) -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt():\n",
    "Preparing an encoding prompt helps create a process for text generation. This process serves as a starting point for the model to generate subsequent tokens. Once this prompt is tokenized, the decoder model can process and generate the next tokens based on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def encode_prompt(prompt, block_size=BLOCK_SIZE):\n",
    "  # Handle None prompt\n",
    "  if prompt is None:\n",
    "    prompt = '<pad>' * block_size\n",
    "  else:\n",
    "    tokens = tokenizer(prompt)\n",
    "    number_of_tokens = len(tokens)\n",
    "    # Adjust prompt length to fit block_size\n",
    "    if number_of_tokens > block_size:\n",
    "      tokens = tokens[-block_size:] # Keep last block_size tokens\n",
    "    elif number_of_tokens < block_size:\n",
    "      padding = ['<pad>'] * (block_size - number_of_tokens)\n",
    "      tokens = padding + tokens # Prepend padding tokens\n",
    "    prompt_indices = vocab(tokens)\n",
    "    prompt_encoded = torch.tensor(prompt_indices, dtype=torch.int64).reshape(-1, 1)\n",
    "  return prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt encoding:\n",
    "Tokenized decoded prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt_encoded=encode_prompt(\"This is a prompt to get model\n",
    "generate next words.\")\n",
    "prompt_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Generate function:\n",
    "The 'generate' function creates autoregressive text in the decoder model.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Autoregressive Language Model text generation\n",
    "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
    "model.to(DEVICE)\n",
    "# Encode the input prompt\n",
    "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "tokens = []\n",
    "# Generate new tokens up to max_new_tokens\n",
    "for _ in range(max_new_tokens):\n",
    "# Decode the encoded prompt using the model's decoder\n",
    "logits = model(prompt_encoded)\n",
    "# Bring the sequence length to the first dimension\n",
    "logits = logits.transpose(0, 1)\n",
    "# Select the logits of the last token in the sequence\n",
    "logit_prediction = logits[:, -1]\n",
    "# Choose the most probable next token from the logits(greedy decoding)\n",
    "next_token_encoded = torch.argmax(logit_prediction,\n",
    "dim=-1).reshape(-1, 1)\n",
    "# If the next token is the end-of-sequence (EOS) token, stop generation\n",
    "if next_token_encoded.item() == EOS_IDX:\n",
    "break\n",
    "# Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "prompt_encoded = torch.cat((prompt_encoded, next_token_encoded),\n",
    "dim=0)[-block_size:]\n",
    "# Convert the next token index to a token string using the vocabulary\n",
    "token_id = next_token_encoded.to('cpu').item()\n",
    "tokens.append(vocab.get_itos()[token_id])\n",
    "# Join the generated tokens into a single string and return\n",
    "return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Generate a token:\n",
    "This function generates text using an autoregressive language model. It iteratively predicts the next token in the sequence based on the previous tokens, using greedy decoding. The generation stops when either the maximum number of new tokens is reached or an end-of-sequence token is predicted. Finally, it returns the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Autoregressive Language Model text generation\n",
    "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
    "model.to(DEVICE)\n",
    "# Encode the input prompt\n",
    "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "tokens = []\n",
    "# Generate new tokens up to max_new_tokens\n",
    "for _ in range(max_new_tokens):\n",
    "# Decode the encoded prompt using the model's decoder\n",
    "logits = model.decoder(prompt_encoded,src_mask=None, key_padding_mask=None)\n",
    "# Bring the sequence length to the first dimension\n",
    "logits = logits.transpose(0, 1)\n",
    "# Select the logits of the last token in the sequence\n",
    "logit_prediction = logits[:, -1]\n",
    "# Choose the most probable next token from the logits(greedy decoding)\n",
    "next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "# If the next token is the end-of-sequence (EOS) token, stop generation\n",
    "if next_token_encoded.item() == EOS_IDX:\n",
    "break\n",
    "# Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "# Convert the next token index to a token string using the vocabulary\n",
    "token_id = next_token_encoded.to('cpu').item()\n",
    "tokens.append(vocab.get_itos()[token_id])\n",
    "# Join the generated tokens into a single string and return\n",
    "return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Generate function:\n",
    "This function generates text using an autoregressive language model. It takes a pretrained model, an optional prompt, and parameters for controlling text generation. It iteratively predicts the next token in the sequence based on the previous tokens. The generation stops when either the maximum number of new tokens is reached or an end-of-sequence token is predicted. Finally, it returns the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Autoregressive Language Model text generation\n",
    "def generate(model, prompt=None, max_new_tokens=500, block_size=BLOCK_SIZE, vocab=vocab, tokenizer=tokenizer):\n",
    "model.to(DEVICE)\n",
    "# Encode the input prompt\n",
    "prompt_encoded = encode_prompt(prompt).to(DEVICE)\n",
    "tokens = []\n",
    "# Generate new tokens up to max_new_tokens\n",
    "for _ in range(max_new_tokens):\n",
    "# Decode the encoded prompt using the model's decoder\n",
    "logits = model.decoder(prompt_encoded,src_mask=None, key_padding_mask=None)\n",
    "# Bring the sequence length to the first dimension\n",
    "logits = logits.transpose(0, 1)\n",
    "# Select the logits of the last token in the sequence\n",
    "logit_prediction = logits[:, -1]\n",
    "# Choose the most probable next token from the logits(greedy decoding)\n",
    "next_token_encoded = torch.argmax(logit_prediction, dim=-1).reshape(-1, 1)\n",
    "# If the next token is the end-of-sequence (EOS) token, stop generation\n",
    "if next_token_encoded.item() == EOS_IDX:\n",
    "break\n",
    "# Append the next token to the prompt_encoded and keep only the last 'block_size' tokens\n",
    "prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=0)[-block_size:]\n",
    "# Convert the next token index to a token string using the vocabulary\n",
    "token_id = next_token_encoded.to('cpu').item()\n",
    "tokens.append(vocab.get_itos()[token_id])\n",
    "# Join the generated tokens into a single string and return\n",
    "return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and vocabulary building:\n",
    "This code sets up the necessary infrastructure for tokenizing text data and converting it into numerical indices, facilitating subsequent NLP tasks like model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "# Define a function to yield tokenized samples\n",
    "def yield_tokens(data_iter):\n",
    "for label, data_sample in data_iter:\n",
    "yield tokenizer(data_sample)\n",
    "# Define special symbols and their indices\n",
    "PAD_IDX, CLS_IDX, SEP_IDX, MASK_IDX, UNK_IDX = 0, 1, 2, 3, 4\n",
    "special_symbols = ['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
    "# Split the data into training and testing sets using the IMDB data set\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "# Build the vocabulary from the training data\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=special_symbols, special_first=True)\n",
    "# Set the default index of the vocabulary to UNK_IDX\n",
    "vocab.set_default_index(UNK_IDX)\n",
    "# Get the size of the vocabulary\n",
    "VOCAB_SIZE = len(vocab)\n",
    "text_to_index=lambda text: [vocab(token) for token in tokenizer(text)]\n",
    "index_to_en = lambda seq_en: \" \".join([vocab.get_itos()[index] for index in seq_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text masking:\n",
    "This code defines a function called Masking(token), which is responsible for applying masking to a token with a certain probability. If the mask decision is false (with an 80% chance), it returns the token with a '[PAD]' label. If masking is applied (with a 20% chance), it randomly selects between three cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "# Decide whether to mask this token (20% chance)\n",
    "mask = bernoulli_true_false(0.2)\n",
    "# If mask is False, immediately return with '[PAD]' label\n",
    "if not mask:\n",
    "return token, '[PAD]'\n",
    "# If mask is True, proceed with further operations\n",
    "# Randomly decide on an operation (50% chance each)\n",
    "random_opp = bernoulli_true_false(0.5)\n",
    "random_swich = bernoulli_true_false(0.5)\n",
    "# Case 1: If mask, random_opp, and random_swich are True\n",
    "if mask and random_opp and random_swich:\n",
    "# Replace the token with '[MASK]' and set label to a random token\n",
    "mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "token_ = '[MASK]'\n",
    "# Case 2: If mask and random_opp are True, but random_swich is False\n",
    "elif mask and random_opp and not random_swich:\n",
    "# Leave the token unchanged and set label to the same token\n",
    "token_ = token\n",
    "mask_label = token\n",
    "# Case 3: If mask is True, but random_opp is False\n",
    "else:\n",
    "# Replace the token with '[MASK]' and set label to the original token\n",
    "token_ = '[MASK]'\n",
    "mask_label = token\n",
    "return token_, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLM preparations:\n",
    "This code defines a function Masking(token), which is responsible for applying masking to a token. It decides whether to mask the token with a 20% chance. If not, it returns the token with a '[PAD]' label. If masking is applied, it randomly chooses between three cases.\n",
    "\n",
    "Case 1: It replaces the token with '[MASK]' and assigns a random token as the label.\n",
    "\n",
    "Case 2: It retains the token unchanged and assigns the same token as the label.\n",
    "\n",
    "Case 3: It replaces the token with '[MASK]' and assigns the original token as the label.\n",
    "\n",
    "The choice between these cases is determined by two independent 50% chances (random_opp and random_swich). Finally, it returns the modified token and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def Masking(token):\n",
    "# Decide whether to mask this token (20% chance)\n",
    "mask = bernoulli_true_false(0.2)\n",
    "# If mask is False, immediately return with '[PAD]' label\n",
    "if not mask:\n",
    "return token, '[PAD]'\n",
    "# If mask is True, proceed with further operations\n",
    "# Randomly decide on an operation (50% chance each)\n",
    "random_opp = bernoulli_true_false(0.5)\n",
    "random_swich = bernoulli_true_false(0.5)\n",
    "# Case 1: If mask, random_opp, and random_swich are True\n",
    "if mask and random_opp and random_swich:\n",
    "# Replace the token with '[MASK]' and set label to a random token\n",
    "mask_label = index_to_en(torch.randint(0, VOCAB_SIZE, (1,)))\n",
    "token_ = '[MASK]'\n",
    "# Case 2: If mask and random_opp are True, but random_swich is False\n",
    "elif mask and random_opp and not random_swich:\n",
    "# Leave the token unchanged and set label to the same token\n",
    "token_ = token\n",
    "mask_label = token\n",
    "# Case 3: If mask is True, but random_opp is False\n",
    "else:\n",
    "# Replace the token with '[MASK]' and set label to the original token\n",
    "token_ = '[MASK]'\n",
    "mask_label = token\n",
    "return token_, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NSP preparations:\n",
    "This code defines a function process_for_nsp that prepares inputs for training BERT for the next sentence prediction (NSP) task. It takes two inputs: input_sentences, a list of sentences, and input_masked_labels, a list of labels corresponding to masked tokens in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_for_nsp(input_sentences, input_masked_labels):\n",
    "# Verify that both input lists are of the same length and have a sufficient number of sentences\n",
    "if len(input_sentences) < 2:\n",
    "raise ValueError(\"Must have two same number of items.\")\n",
    "if len(input_sentences) != len(input_masked_labels):\n",
    "raise ValueError(\"Both lists must have the same number of\n",
    "items.\")\n",
    "bert_input = []\n",
    "bert_label = []\n",
    "is_next = []\n",
    "available_indices = list(range(len(input_sentences)))\n",
    "while len(available_indices) >= 2:\n",
    "if random.random() < 0.5:\n",
    "# Choose two consecutive sentences to simulate the 'next sentence' scenario\n",
    "index = random.choice(available_indices[:-1]) # Exclude the last index\n",
    "# append list and add '[CLS]' and '[SEP]' tokens\n",
    "bert_input.append([['[CLS]']+input_sentences[index]+['[SEP]'],input_sentences[index + 1]+['[SEP]']])\n",
    "bert_label.append([['[PAD]']+input_masked_labels[index]+['[PAD]'], input_masked_labels[index + 1]+['[PAD]']])\n",
    "is_next.append(1) # Label 1 indicates these sentences are consecutive\n",
    "# Remove the used indices\n",
    "available_indices.remove(index)\n",
    "if index + 1 in available_indices:\n",
    "available_indices.remove(index + 1)\n",
    "else:\n",
    "# Choose two random distinct sentences to simulate the 'not next sentence' scenario\n",
    "indices = random.sample(available_indices, 2)\n",
    "bert_input.append([['[CLS]']+input_sentences[indices[0]]+['[SEP]'],input_sentences[indices[1]]+['[SEP]']])\n",
    "bert_label.append([['[PAD]']+input_masked_labels[indices[0]]+['[PAD]'], input_masked_labels[indices[1]]+['[PAD]']])\n",
    "is_next.append(0) # Label 0 indicates these sentences are not\n",
    "consecutive\n",
    "# Remove the used indices\n",
    "available_indices.remove(indices[0])\n",
    "available_indices.remove(indices[1])\n",
    "return bert_input, bert_label, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training-ready inputs for BERT:\n",
    "This code defines a function prepare_bert_final_inputs, which prepares inputs for training a BERT model. It takes bert_inputs, bert_labels, and is_nexts as inputs. The function pads the inputs and labels with [PAD] tokens to ensure they are of equal length, creates segment labels for each pair of sentences, and converts the inputs, labels, and segment labels into tensors if to_tensor is set to True. Finally, it returns the processed inputs, labels, segment labels, and is_nexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts,\n",
    "to_tensor=True):\n",
    "def zero_pad_list_pair(pair_, pad='[PAD]'):\n",
    "pair = deepcopy(pair_)\n",
    "max_len = max(len(pair[0]), len(pair[1]))\n",
    "# Append [PAD] to each sentence in the pair until the maximum length is reached\n",
    "pair[0].extend([pad] * (max_len - len(pair[0])))\n",
    "pair[1].extend([pad] * (max_len - len(pair[1])))\n",
    "return pair[0], pair[1]\n",
    "# Flatten the tensor\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# Transform tokens to vocab indices\n",
    "tokens_to_index = lambda tokens: [vocab[token] for token in tokens]\n",
    "bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final = [], [], [], []\n",
    "for bert_input, bert_label, is_next in zip(bert_inputs, bert_labels, is_nexts):\n",
    "# Create segment labels for each pair of sentences\n",
    "segment_label = [[1] * len(bert_input[0]), [2] * len(bert_input[1])]\n",
    "# Zero-pad the bert_input, bert_label, and segment_label\n",
    "bert_input_padded = zero_pad_list_pair(bert_input)\n",
    "bert_label_padded = zero_pad_list_pair(bert_label)\n",
    "segment_label_padded = zero_pad_list_pair(segment_label, pad=0)\n",
    "# Convert to tensors\n",
    "if to_tensor:\n",
    "# Flatten the padded inputs and labels, transform tokens to their corresponding vocab indices, and convert them to tensors\n",
    "bert_inputs_final.append(torch.tensor(tokens_to_index(flatten(bert_input_padded)), dtype=torch.int64))\n",
    "bert_labels_final.append(torch.tensor(tokens_to_index(flatten(bert_label_padded)), dtype=torch.int64))\n",
    "segment_labels_final.append(torch.tensor(flatten(segment_label_padded), dtype=torch.int64))\n",
    "is_nexts_final.append(is_next)\n",
    "else:\n",
    "# Flatten the padded inputs and labels\n",
    "bert_inputs_final.append(flatten(bert_input_padded))\n",
    "bert_labels_final.append(flatten(bert_label_padded))\n",
    "segment_labels_final.append(flatten(segment_label_padded))\n",
    "is_nexts_final.append(is_next)\n",
    "return bert_inputs_final, bert_labels_final, segment_labels_final, is_nexts_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training-ready CSV file from IMDB:\n",
    "This code writes the processed Internet Movie Database or IMDB data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "csv_file_path = 'train_bert_data_new.csv'\n",
    "# Open the CSV file for writing\n",
    "with open(csv_file_path, mode='w', newline='', encoding='utf-8') as\n",
    "file:\n",
    "csv_writer = csv.writer(file)\n",
    "# Write the header row\n",
    "csv_writer.writerow(['Original Text', 'BERT Input', 'BERT Label', 'Segment Label', 'Is Next'])\n",
    "# Wrap train_iter with tqdm for a progress bar\n",
    "for n, (_, sample) in enumerate(tqdm(train_iter, desc=\"Processing samples\")):\n",
    "# Tokenize the sample input\n",
    "tokens = tokenizer(sample)\n",
    "# Create MLM inputs and labels\n",
    "bert_input, bert_label = prepare_for_mlm(tokens, include_raw_tokens=False)\n",
    "# Skip samples with insufficient input length\n",
    "if len(bert_input) < 2:\n",
    "continue\n",
    "# Create NSP pairs, token labels, and is_next label\n",
    "bert_inputs, bert_labels, is_nexts = process_for_nsp(bert_input, bert_label)\n",
    "# Add zero-paddings, map tokens to vocab indices, and create segment labels\n",
    "bert_inputs, bert_labels, segment_labels, is_nexts = prepare_bert_final_inputs(bert_inputs, bert_labels, is_nexts)\n",
    "# Convert tensors to lists and then convert lists to JSON-formatted strings\n",
    "for bert_input, bert_label, segment_label, is_next in\n",
    "zip(bert_inputs, bert_labels, segment_labels, is_nexts):\n",
    "bert_input_str = json.dumps(bert_input.tolist())\n",
    "bert_label_str = json.dumps(bert_label.tolist())\n",
    "segment_label_str = ','.join(map(str, segment_label.tolist()))\n",
    "# Write the data to a CSV file row-by-row\n",
    "csv_writer.writerow([sample, bert_input_str, bert_label_str, segment_label_str, is_next])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__:\n",
    "Used to initialize objects of a class. It is also called a constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__len__:\n",
    "Essentially used to implement the built-in len() function. Whenever you call len(), Python internally invokes the __len__ magic method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def __len__(self):\n",
    "return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__getitem__:\n",
    "Used to define the behavior of retrieving items from an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def __getitem__(self, idx):\n",
    "return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor(...):\n",
    "Creates a PyTorch tensor from the Python object obtained from the JSON string. It converts the Python object into a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor(json.loads(row['BERT Input'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is_next:\n",
    "A PyTorch tensor created from a value stored in a DataFrame row. Specifically, it's created from the value associated with the key 'Is Next'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "is_next = torch.tensor(row['Is Next'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collate_batch:\n",
    "Responsible for collating individual samples into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "label_list, text_list, lengths = [], [], []\n",
    "for _label, _text in batch:\n",
    "label_list.append(label_pipeline(_label))\n",
    "processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "text_list.append(processed_text)\n",
    "lengths.append(processed_text.size(0))\n",
    "if CONFIG_USE_ROCM:\n",
    "label_list = torch.tensor(label_list, device='cuda')\n",
    "lengths = torch.tensor(lengths, device='cuda')\n",
    "else:\n",
    "label_list = torch.tensor(label_list)\n",
    "lengths = torch.tensor(lengths)\n",
    "padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)\n",
    "padded_text_list.to('cuda')\n",
    "#code.interact(local=locals())\n",
    "return padded_text_list, label_list, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward:\n",
    "Defines the forward pass computation, which includes applying the various embedding layers and dropout during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, bert_inputs, segment_labels=False):\n",
    "my_embeddings = self.token_embedding(bert_inputs)\n",
    "if self.train:\n",
    "x = self.dropout(my_embeddings + self.positional_encoding(my_embeddings) + self.segment_embedding(segment_labels))\n",
    "else:\n",
    "x = my_embeddings + self.positional_encoding(my_embeddings)\n",
    "return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.no_grad():\n",
    "Context manager provided by PyTorch that turns off gradients during validation or evaluation to save memory and computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad(): # Turning off gradients for validation saves memory and computations\n",
    "for batch in dataloader:\n",
    "bert_inputs, bert_labels, segment_labels, is_nexts = [b.to(device) for b in batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate:\n",
    "Used for evaluating the BERT model's performance on the test data set. It calculates the average loss over all batches in the test data set and prints the average loss, average next sentence loss, and average mask loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def evaluate(dataloader=test_dataloader, model=model, loss_fn=loss_fn, device=device):\n",
    "model.eval() # Turn off dropout and other training-specific behaviors\n",
    "total_loss = 0\n",
    "total_next_sentence_loss = 0\n",
    "total_mask_loss = 0\n",
    "total_batches = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam:\n",
    "Initializes the Adam optimizer, which is a variant of stochastic gradient descent (SGD). It's commonly used for optimizing neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=0.01, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero_grad():\n",
    "Used to zero out the gradients of all parameters of the model. It's typically called before performing the backward pass to avoid accumulating gradients from previous iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "def linear_model(x, W, b):\n",
    "return torch.matmul(x, W) + b\n",
    "data, targets = ...\n",
    "a = Variable(torch.randn(4, 3), requires_grad=True)\n",
    "b = Variable(torch.randn(3), requires_grad=True)\n",
    "optimizer = optim.Adam([a, b])\n",
    "for sample, target in zip(data, targets):\n",
    "optimizer.zero_grad()\n",
    "output = linear_model(sample, W, b)\n",
    "loss = (output - target) ** 2\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward():\n",
    "Computes gradients of the loss with respect to the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "def linear_model(x, W, b):\n",
    "return torch.matmul(x, W) + b\n",
    "data, targets = ...\n",
    "a = Variable(torch.randn(4, 3), requires_grad=True)\n",
    "b = Variable(torch.randn(3), requires_grad=True)\n",
    "optimizer = optim.Adam([a, b])\n",
    "for sample, target in zip(data, targets):\n",
    "optimizer.zero_grad()\n",
    "output = linear_model(sample, W, b)\n",
    "loss = (output - target) ** 2\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn.utils.clip_grad_norm_:\n",
    "Used for gradient clipping, which is a technique to prevent the exploding gradient problem during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step():\n",
    "Updates the parameters of the model using the gradients computed during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save:\n",
    "Used to save the model's state dictionary to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot:\n",
    "Used to plot data points on a graph. It takes the x-values, y-values, and optional arguments to customize the plot, such as line style, color, and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.xlabel:\n",
    "Used to set the label for the x-axis of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.ylabel:\n",
    "Used to set the label for the y-axis of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.title:\n",
    "Used to set the title of the plot. It specifies the text that will be displayed as the title above the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lt.title('Training and Evaluation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.legend:\n",
    "Used to add a legend to the plot. It displays labels associated with each plot line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.show:\n",
    "Used to display the plot on the screen or in the output of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_nsp:\n",
    "A function that takes two sentences, a BERT model, and a tokenizer as input. It tokenizes the input sentences using the tokenizer, then feeds the tokenized inputs to the BERT model to predict whether the second sentence follows the first one (Next Sentence Prediction task). The function returns a string indicating whether the second sentence follows the first one or not based on the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sentence1 = \"The cat is sitting on the chair.\"\n",
    "sentence2 = \"It is a rainy day\"\n",
    "print(predict_nsp(sentence1, sentence2, model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_mlm:\n",
    "Takes an input sentence, a BERT model, and a tokenizer as input. It tokenizes the input sentence using the tokenizer and converts it into token IDs. Then, it creates dummy segment labels filled with zeros and feeds the input tokens and segment labels to the BERT model. The function extracts the position of the [MASK] token and retrieves the predicted index for the [MASK] token from the model's predictions. Finally, it replaces the [MASK] token in the original sentence with the predicted token and returns the predicted sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_mlm(sentence, model, tokenizer):\n",
    "# Tokenize the input sentence and convert to token IDs, including special tokens\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "tokens_tensor = inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_square_subsequent_mask:\n",
    "Generates a square subsequent mask for self-attention mechanisms in transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n",
    "mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_mask:\n",
    "Creates masks for the source and target sequences, as well as padding masks for both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_mask(src, tgt,device=DEVICE):\n",
    "src_seq_len = src.shape[0]\n",
    "tgt_seq_len = tgt.shape[0]\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode:\n",
    "Responsible for encoding the input source sequence into a fixed-dimensional representation that captures the contextual information of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def encode(self, src: Tensor, src_mask: Tensor):\n",
    "src_embedded = self.src_tok_emb(src)\n",
    "src_pos_encoded = self.positional_encoding(src_embedded)\n",
    "return self.transformer.encoder(src_pos_encoded, src_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decode:\n",
    "Generates the output sequence based on the encoded source sequence and the target sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "tgt_embedded = self.tgt_tok_emb(tgt)\n",
    "tgt_pos_encoded = self.positional_encoding(tgt_embedded)\n",
    "return self.transformer.decoder(tgt_pos_encoded, memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_epoch:\n",
    "Represents a training epoch in the training loop. It takes the model, optimizer, and training dataloader as input arguments and returns the average loss over the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer,train_dataloader):\n",
    "model.train()\n",
    "losses = 0\n",
    "for src, tgt in train_dataloader:\n",
    "src = src.to(DEVICE)\n",
    "tgt = tgt.to(DEVICE)\n",
    "tgt_input = tgt[:-1, :]\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "src_mask = src_mask.to(DEVICE)\n",
    "tgt_mask= tgt_mask.to(DEVICE)\n",
    "src_padding_mask= src_padding_mask.to(DEVICE)\n",
    "tgt_padding_mask =tgt_padding_mask.to(DEVICE)\n",
    "logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "logits = logits.to(DEVICE)\n",
    "optimizer.zero_grad()\n",
    "tgt_out = tgt[1:, :]\n",
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.\n",
    "Reshape(-1))\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "losses += loss.item()\n",
    "return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy_decode:\n",
    "Performs greedy decoding to generate an output sequence using the trained transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "src = src.to(DEVICE)\n",
    "src_mask = src_mask.to(DEVICE)\n",
    "memory = model.encode(src, src_mask)\n",
    "ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "for i in range(max_len-1):\n",
    "memory = memory.to(DEVICE)\n",
    "tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "out = model.decode(ys, memory, tgt_mask)\n",
    "out = out.transpose(0, 1)\n",
    "prob = model.generator(out[:, -1])\n",
    "_, next_word = torch.max(prob, dim=1)\n",
    "next_word = next_word.item()\n",
    "ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "if next_word == EOS_IDX:\n",
    "break\n",
    "return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "translate(model: torch.nn.Module, src_sentence: str):\n",
    "Translates a given source sentence into the target language using the provided PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "model.eval()\n",
    "src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "num_tokens = src.shape[0]\n",
    "src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens +\n",
    "5, start_symbol=BOS_IDX).flatten()\n",
    "return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

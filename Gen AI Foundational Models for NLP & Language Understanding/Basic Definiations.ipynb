{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words:\n",
    "A representation that portrays a document as the aggregate or average of one-hot encoded vectors. It represents documents as a set of words and considers the frequency of a word's occurrence within the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bi-gram model:\n",
    "A conditional probability model with context size one, which means that you consider only the adjacent words  in the sequence to predict the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context vector:\n",
    "Product of the context size and the size of the vocabulary. Typically, this vector is not computed directly but is constructed by concatenating the embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous bag of words (CBOW):\n",
    "A model that utilizes context words to predict a target word and generate its embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss:\n",
    "A metric used to measure the performance of a classification model. The output is a number between 0 and 1. The smaller the number, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader:\n",
    "Application component that enables efficient batching and shuffling of data, which is essential for training neural networks. It allows for on-the-fly preprocessing, which optimizes memory usage. Data loaders are important for managing large data sets efficiently during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set:\n",
    "A collection of data samples and their labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer:\n",
    "A layer that accepts token indices and produces embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning:\n",
    "Adjusting a pretrained model to improve performance for a specific task or data set. This makes the model generate more accurate and contextually relevant content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated recurrent units (or GRUs):\n",
    "A popular recurrent neural network (RNN) enhancements with a gating mechanism to control information flow within the network. They are similar to long short-term memory (LSTM) but can be trained quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters:\n",
    "Configuration settings of a neural network that are external to a model and define aspects such as behavior during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnable parameters:\n",
    "The weights and biases in a neural network that are optimized during the training of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate:\n",
    "A hyperparameter that determines how quickly or slowly the neural network learns from the data. It regulates the step size in the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits:\n",
    "Raw, unnormalized outputs of a neural network before the activation function is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (or LSTMs):\n",
    "A popular recurrent neural network (RNN) enhancements effective for tasks involving extensive time-series data, such as natural language processing (NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "A measure that represents the difference between the values predicted by a model and the actual values in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo sampling:\n",
    "A statistical technique that involves generating random samples from a probability distribution. It is specifically beneficial when dealing with systems that involve uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP):\n",
    "The subfield of artificial intelligence (AI) that deals with the interaction of computers and humans in human language. It involves creating algorithms and models that will help computers understand and comprehend human language and generate contextually relevant text in human language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks:\n",
    "Computational models inspired by the structure of the human brain. A neural network model consists of an input layer, one or more hidden layers, and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram model:\n",
    "Language model that analyzes sequences of 'n' consecutive items, often words, to predict patterns or phrases occurring in a text. The n-gram model allows for an arbitrary context size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK:\n",
    "A Python library used in natural language processing (NLP) for tasks, such as tokenization and text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding:\n",
    "The method used to convert categorical data into feature vectors that a neural network can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity:\n",
    "Metric for evaluating the efficiency of large language models (LLMs) and generative AI models. In language modeling, perplexity can be seen as a measure of how surprised or uncertain the model is when predicting the next word in a sequence.  Lower perplexity values indicate better performance of language models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

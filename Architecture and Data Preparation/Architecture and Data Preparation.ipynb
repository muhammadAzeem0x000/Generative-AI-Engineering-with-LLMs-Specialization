{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK:\n",
    "NLTK is a Python library used in natural language processing (NLP) for tasks such as tokenization and text processing. The code example shows how you can tokenize text using the NLTK word-based tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK implementation example\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\"\n",
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy:\n",
    "spaCy is an open-source library used in NLP. It provides tools for tasks such as tokenization and word embeddings. The code example shows how you can tokenize text using spaCy word-based tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "text = \"Unicorns are real. I saw a unicorn yesterday. I couldn't see it today.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertTokenizer:\n",
    "BertTokenizer is a subword-based tokenizer that uses the WordPiece algorithm. The code example shows how you can tokenize text using BertTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XLNetTokenizer:\n",
    "XLNetTokenizer tokenizes text using Unigram and SentencePiece algorithms. The code example shows how you can tokenize text using XLNetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext:\n",
    "The torchtext library is part of the PyTorch ecosystem and provides the tools and functionalities required for NLP. The code example shows how you can use torchtext to generate tokens and convert them to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "# Defines a dataset\n",
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\"NLP Named Entity,Sentiment Analysis, Machine Translation\"),\n",
    "    (1,\"Machine Translation with NLP\"),\n",
    "    (1,\"Named Entity vs Sentiment Analysis NLP\")]\n",
    "# Applies the tokenizer to the text to get the tokens as a list\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(dataset[0][1])\n",
    "# Takes a data iterator as input, processes text from the iterator, \n",
    "# and yields the tokenized output individually\n",
    "def yield_tokens(data_iter):\n",
    "    for _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "# Creates an iterator\n",
    "my_iterator = yield_tokens(dataset)\n",
    "# Fetches the next set of tokens from the data set\n",
    "next(my_iterator)\n",
    "# Converts tokens to indices and sets <unk> as the \n",
    "# default word if a word is not found in the vocabulary\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# Gives a dictionary that maps words to their corresponding numerical indices\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab:\n",
    "The vocab object is part of the PyTorch torchtext library. It maps tokens to indices. The code example shows how you can apply the vocab object to tokens directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. \n",
    "# Creates a list of token indices using the vocab dictionary for each token.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices\n",
    "# Returns the tokenized sentences and the corresponding token indices. \n",
    "# Repeats the process.\n",
    "tokenized_sentence, token_indices = \\\n",
    "get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "# Prints the tokenized sentence and its corresponding token indices.\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in PyTorch: <eos> and <bos>:\n",
    "Special tokens are tokens introduced to input sequences to convey specific information or serve a particular purpose during training. The code example shows the use of <bos> and <eos> during tokenization. The <bos> token denotes the beginning of the input sequence, and the <eos> token denotes the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends <bos> at the beginning and <eos> at the end of the tokenized sentences \n",
    "# using a loop that iterates over the sentences in the input data\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "max_length = 0\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in PyTorch: <pad>:\n",
    "The code example shows the use of <pad> token to ensure all sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads the tokenized lines\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset class in PyTorch:\n",
    "The Dataset class enables accessing and retrieving individual samples from a data set. The code example shows how you can create a custom data set and access samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Dataset class and defines a list of sentences\n",
    "from torch.utils.data import Dataset\n",
    "sentences = [\"If you want to know what a man's like, take a \n",
    "good look at how he treats his inferiors, not his equals.\", \n",
    "\"Fae's a fickle friend, Harry.\"]\n",
    "# Downloads and reads data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences):\n",
    "        self.sentences = sentences\n",
    "    # Returns the data length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    # Returns one item on the index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx]\n",
    "# Creates a dataset object\n",
    "dataset=CustomDataset(sentences)\n",
    "# Accesses samples like in a list\n",
    "E.g., dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader class in PyTorch:\n",
    "A DataLoader class enables efficient loading and iteration over data sets for training deep learning models. The code example shows how you can use the DataLoader class to generate batches of sentences for further processing, such as training a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an iterator object\n",
    "data_iter = iter(dataloader)\n",
    "# Calls the next function to return new batches of samples\n",
    "next(data_iter)\n",
    "# Creates an instance of the custom data set\n",
    "from torch.utils.data import DataLoader\n",
    "custom_dataset = CustomDataset(sentences)\n",
    "# Specifies a batch size\n",
    "batch_size = 2\n",
    "# Creates a data loader\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Prints the sentences in each batch\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom collate function in PyTorch:\n",
    "The custom collate function is a user-defined function that defines how individual samples are collated or batched together. You can utilize the collate function for tasks such as tokenization, converting tokenized indices, and transforming the result into a tensor. The code example shows how you can use a custom collate function in a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a custom collate function\n",
    "def collate_fn(batch):\n",
    "    tensor_batch = []\n",
    "# Tokenizes each sample in the batch\n",
    "    for sample in batch:\n",
    "        tokens = tokenizer(sample)\n",
    "# Maps tokens to numbers using the vocab\n",
    "        tensor_batch.append(torch.tensor([vocab[token] for token in tokens]))\n",
    "# Pads the sequences within the batch to have equal lengths\n",
    "    padded_batch = pad_sequence(tensor_batch,batch_first=True)\n",
    "    return padded_batch\n",
    "# Creates a data loader using the collate function and the custom dataset\n",
    "dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a data set\n",
    "dataset = [\n",
    "\"I like cats\",\n",
    "\"I hate dogs\",\n",
    "\"I'm impartial to hippos\"\n",
    "]\n",
    "#Initializing the tokenizer, iterator from the data set, and vocabulary\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "def yield_tokens(data_iter):\n",
    "    for data_sample in data_iter:\n",
    "        yield tokenizer(data_sample)\n",
    "data_iter = iter(dataset)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data_iter))\n",
    "#Tokenizing and generating indices\n",
    "input_ids=lambda x:[torch.tensor(vocab(tokenizer(data_sample))) for data_sample in dataset]\n",
    "index=input_ids(dataset)\n",
    "print(index)\n",
    "#Initiating the embedding layer, specifying the dimension size for the embeddings, \n",
    "#determining the count of unique tokens present in the vocabulary, and creating the embedding layer\n",
    "embedding_dim = 3\n",
    "n_embedding = len(vocab)\n",
    "n_embedding:9\n",
    "embeds = nn.Embedding(n_embedding, embedding_dim)\n",
    "#Applying the embedding object\n",
    "i_like_cats=embeds(index[0])\n",
    "i_like_cats\n",
    "impartial_to_hippos=embeds(index[-1])\n",
    "impartial_to_hippos\n",
    "#Initializing the embedding bag layer\n",
    "embedding_dim = 3\n",
    "n_embedding = len(vocab)\n",
    "n_embedding:9\n",
    "embedding_bag = nn.EmbeddingBag(n_embedding, embedding_dim)\n",
    "# Output the embedding bag\n",
    "dataset = [\"I like cats\",\"I hate dogs\",\"I'm impartial to hippos\"]\n",
    "index:[tensor([0, 7, 2]), tensor([0, 4, 3]), tensor([0, 1, 6, 8, 5])]\n",
    "i_like_cats=embedding_bag(index[0],offsets=torch.tensor([0]))\n",
    "i_like_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch/Embedding and EmbeddingBag:\n",
    "Embedding is a class that represents an embedding layer. It accepts token indices and produces embedding vectors. EmbeddingBag is a class that aggregates embeddings using mean or sum operations. Embedding and EmbeddingBag are part of the torch.nn module. The code example above shows how you can use Embedding and EmbeddingBag in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch function:\n",
    "Defines the number of samples that will be propagated through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "target_list, context_list, offsets = [], [], [0]\n",
    "for _context, _target in batch:\n",
    "target_list.append(vocab[_target]) \n",
    "processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)\n",
    "context_list.append(processed_context)\n",
    "offsets.append(processed_context.size(0))\n",
    "target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "context_list = torch.cat(context_list)\n",
    "return target_list.to(device), context_list.to(device), offsets.to(device)\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "dataloader_cbow = DataLoader(cobw_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass:\n",
    "Refers to the computation and storage of intermediate variables (including outputs) for a neural network in order from the input to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, text):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford's pre-trained GloVe:\n",
    "Leverages large-scale data for word embeddings. It can be integrated into PyTorch for improved NLP tasks such as classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe,vocab\n",
    "# Creating an instance of the 6B version of Glove() model\n",
    "glove_vectors_6B = GloVe(name ='6B') # you can specify the model with the following format: GloVe(name='840B', dim=300)\n",
    "# Build vocab from glove_vectors\n",
    "vocab = vocab(glove_vectors_6B.stoi, 0,specials=('<unk>', '<pad>'))\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab:\n",
    "The vocab object is part of the PyTorch torchtext library. It maps tokens to indices. The code example shows how you can apply the vocab object to tokens directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes an iterator as input and extracts the next tokenized sentence. Creates a list of token indices using the vocab dictionary for each token.\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "    return tokenized_sentence, token_indices\n",
    "# Returns the tokenized sentences and the corresponding token indices. Repeats the process.\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "# Prints the tokenized sentence and its corresponding token indices.\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in PyTorch: <eos> and <bos>:\n",
    "Tokens introduced to input sequences to convey specific information or serve a particular purpose during training. The code example shows the use of <bos> and <eos> during tokenization. The <bos> token denotes the beginning of the input sequence, and the <eos> token denotes the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appends <bos> at the beginning and <eos> at the end of the tokenized sentences \n",
    "# using a loop that iterates over the sentences in the input data\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = []\n",
    "max_length = 0\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special tokens in PyTorch: <pad>:\n",
    "Tokens introduced to input sequences to convey specific information or serve a particular purpose during training. The code example shows the use of <pad> token to ensure all sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads the tokenized lines\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy loss:\n",
    "A metric used in machine learning (ML) to evaluate the performance of a classification model. The loss is measured as the probability value between 0 (perfect model) and 1. Typically, the aim is to bring the model as close to 0 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "model = TextClassificationModel(vocab_size,emsize,num_class)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "predicted_label = model(text, offsets)\n",
    "loss = criterion(predicted_label, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization:\n",
    "Method to reduce losses in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an iterator object\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "optimizer.zero_grad()\n",
    "predicted_label = model(text, offsets)\n",
    "loss = criterion(predicted_label, label)\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence_bleu():\n",
    "NLTK (or Natural Language Toolkit) provides this function to evaluate a hypothesis sentence against one or more reference sentences. The reference sentences must be presented as a list of sentences where each reference is a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def calculate_bleu_score(generated_translation, reference_translations):\n",
    "# Convert the generated translations and reference translations into the expected format for sentence_bleu\n",
    "references = [reference.split() for reference in reference_translations]\n",
    "hypothesis = generated_translation.split()\n",
    "# Calculate the BLEU score\n",
    "bleu_score = sentence_bleu(references, hypothesis)\n",
    "return bleu_score\n",
    "reference_translations = [\"Asian man sweeping the walkway .\",\"An asian man sweeping the walkway .\",\"An Asian man sweeps the sidewalk .\",\"An Asian man is sweeping the sidewalk .\",\"An asian man is sweeping the walkway .\",\"Asian man sweeping the sidewalk .\"]\n",
    "bleu_score = calculate_bleu_score(generated_translation, reference_translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder RNN model:\n",
    "The encoder-decoder seq2seq model works together to transform an input sequence into an output sequence. Encoder is a series of RNNs that process the input sequence individually, passing their hidden states to their next RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "super().__init__()\n",
    "self.hid_dim = hid_dim\n",
    "self.n_layers = n_layers\n",
    "self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
    "self.dropout = nn.Dropout(dropout_prob)\n",
    "def forward(self, input_batch):\n",
    "embed = self.dropout(self.embedding(input_batch))\n",
    "embed = embed.to(device)\n",
    "outputs, (hidden, cell) = self.lstm(embed)\n",
    "return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder RNN model:\n",
    "The encoder-decoder seq2seq model works together to transform an input sequence into an output sequence. The decoder module is a series of RNNs that autoregressively generates the translation as one token at a time. Each generated token goes back into the next RNN along with the hidden state to generate the next token of the output sequence until the end token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "super().__init__()\n",
    "self.output_dim = output_dim\n",
    "self.hid_dim = hid_dim\n",
    "self.n_layers = n_layers\n",
    "self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "self.softmax = nn.LogSoftmax(dim=1)\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "def forward(self, input, hidden, cell):\n",
    "input = input.unsqueeze(0)\n",
    "embedded = self.dropout(self.embedding(input))\n",
    "output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "prediction_logit = self.fc_out(output.squeeze(0))\n",
    "prediction = self.softmax(prediction_logit)\n",
    "return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram model:\n",
    "Predicts surrounding context words from a specific target word. It predicts one context word at a time from a target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram_Model(nn.Module):\n",
    "def __init__(self, vocab_size, embed_dim):\n",
    "super(SkipGram_Model, self).__init__()\n",
    "# Define the embeddings layer\n",
    "self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "# Define the fully connected layer\n",
    "self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)\n",
    "# Perform the forward pass\n",
    "def forward(self, text):\n",
    "# Pass the input text through the embeddings layer\n",
    "out = self.embeddings(text)\n",
    "# Pass the output of the embeddings layer through the fully connected layer\n",
    "# Apply the ReLU activation function\n",
    "out = torch.relu(out)\n",
    "out = self.fc(out)\n",
    "    return out\n",
    "model_sg = SkipGram_Model(vocab_size, emsize).to(device)\n",
    "# Sequence generation function\n",
    "CONTEXT_SIZE = 2\n",
    "skip_data = []\n",
    "for i in range(CONTEXT_SIZE, len(tokenized_toy_data) - CONTEXT_SIZE):\n",
    "    context = (\n",
    "    [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)] # Preceding words\n",
    "    + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)] # Succeeding words)\n",
    "    target = tokenized_toy_data[i]\n",
    "    skip_data.append((target, context))\n",
    "skip_data=[('i', ['wish', 'i', 'was', 'little']), ('was', ['i', 'wish', 'little', 'bit'])],.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collate_fn:\n",
    "Processes the list of samples to form a batch. The batch argument is a list of all your samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    target_list, context_list = [], []\n",
    "    for _context, _target in batch:\n",
    "        target_list.append(vocab[_target])\n",
    "        context_list.append(vocab[_context])\n",
    "        target_list = torch.tensor(target_list, dtype=torch.int64)\n",
    "        context_list = torch.tensor(context_list, dtype=torch.int64)\n",
    "    return target_list.to(device), context_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training function:\n",
    "Trains the model for a specified number of epochs. It also includes a condition to check whether the input is for skip-gram or CBOW. The output of this function includes the trained model and a list of average losses for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):\n",
    "# List to store running loss for each epoch\n",
    "epoch_losses = []\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Storing running loss values for the current epoch\n",
    "running_loss = 0.0\n",
    "# Using tqdm for a progress bar\n",
    "for idx, samples in enumerate(dataloader):\n",
    "optimizer.zero_grad()\n",
    "# Check for EmbeddingBag layer in the model CBOW\n",
    "if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):\n",
    "target, context, offsets = samples\n",
    "predicted = model(context, offsets)\n",
    "# Check for Embedding layer in the model skip gram\n",
    "elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):\n",
    "target, context = samples\n",
    "predicted = model(context)\n",
    "loss = criterion(predicted, target)\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "optimizer.step()\n",
    "running_loss += loss.item()\n",
    "# Append average loss for the epoch\n",
    "epoch_losses.append(running_loss / len(dataloader))\n",
    "return model, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW model:\n",
    "Utilizes context words to predict a target word and generate its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "# Initialize the CBOW model\n",
    "def __init__(self, vocab_size, embed_dim, num_class):\n",
    "super(CBOW, self).__init__()\n",
    "# Define the embedding layer using nn.EmbeddingBag\n",
    "self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "# Define the fully connected layer\n",
    "self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "def forward(self, text, offsets):\n",
    "# Pass the input text and offsets through the embedding layer\n",
    "out = self.embedding(text, offsets)\n",
    "# Apply the ReLU activation function to the output of the first linear layer\n",
    "out = torch.relu(out)\n",
    "# Pass the output of the ReLU activation through the fully connected layer\n",
    "return self.fc(out)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 24\n",
    "model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:\n",
    "Enumerates data from the DataLoader and, on each pass of the loop, gets a batch of training data from the DataLoader, zeros the optimizer's gradients, and performs an inference (gets predictions from the model for an input batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    model.train()\n",
    "    cum_loss=0\n",
    "    for idx, (label, text, offsets) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        cum_loss+=loss.item()\n",
    "    cum_loss_list.append(cum_loss)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    acc_epoch.append(accu_val)\n",
    "    if accu_val > acc_old:\n",
    "        acc_old= accu_val\n",
    "        torch.save(model.state_dict(), 'my_model.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
